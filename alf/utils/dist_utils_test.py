# Copyright (c) 2019 Horizon Robotics. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from absl import logging
from absl.testing import parameterized
from collections import namedtuple
import math
import torch
import torch.distributions as td
from functools import partial

import alf
from alf.utils import math_ops
import alf.utils.dist_utils as dist_utils
from alf.networks.normalizing_flow_networks import _RealNVPTransform
from alf.networks import NetworkWrapper

ActionDistribution = namedtuple('ActionDistribution', ['a', 'b'])


class EstimatedEntropyTest(parameterized.TestCase, alf.test.TestCase):
    def assertArrayAlmostEqual(self, x, y, eps):
        self.assertLess((x - y).abs().max(), eps)

    def test_estimated_entropy(self):
        num_samples = 5000000
        batch_shape = (2, 1)
        # +1 to make sure the distribution shape is well defined
        para1 = torch.rand(*batch_shape) + 1.
        para2 = torch.rand(*batch_shape) + 1.

        dist_ctors = [td.Normal, td.Beta]
        for ctor in dist_ctors:
            dist = td.Independent(
                ctor(para1, para2), reinterpreted_batch_ndims=1)
            exact_entropy = dist.entropy()
            estimated_entropy, _ = dist_utils.estimated_entropy(
                dist, num_samples=num_samples)
            self.assertArrayAlmostEqual(
                exact_entropy, estimated_entropy, eps=1e-2)


class DistributionSpecTest(parameterized.TestCase, alf.test.TestCase):
    def test_normal(self):
        dist = td.Normal(
            loc=torch.tensor([1., 2.]), scale=torch.tensor([0.5, 0.25]))
        spec = dist_utils.DistributionSpec.from_distribution(dist)
        params1 = {
            'loc': torch.tensor([0.5, 1.5]),
            'scale': torch.tensor([2., 4.])
        }
        dist1 = spec.build_distribution(params1)
        self.assertEqual(type(dist1), td.Normal)
        self.assertEqual(dist1.mean, params1['loc'])
        self.assertEqual(dist1.stddev, params1['scale'])

    def test_categorical(self):
        dist = td.Categorical(logits=torch.tensor([1., 2.]))
        spec = dist_utils.DistributionSpec.from_distribution(dist)
        params1 = {'logits': torch.tensor([0.5, 1.5])}
        dist1 = spec.build_distribution(params1)
        self.assertEqual(type(dist1), td.Categorical)
        # Categorical distribution will substract logsumexp(logits) from logits.
        # So dist1.logits is not equal to the supplied logits
        d = dist1.logits - params1['logits']
        self.assertAlmostEqual(d[0], d[1])

    def test_diag_multivariate_normal(self):
        dist = dist_utils.DiagMultivariateNormal(
            torch.tensor([[1., 2.], [2., 2.]]),
            torch.tensor([[2., 3.], [1., 1.]]))
        spec = dist_utils.DistributionSpec.from_distribution(dist)

        params1 = {
            'loc': torch.tensor([[0.5, 1.5], [1.0, 1.0]]),
            'scale': torch.tensor([[2., 4.], [2., 1.]])
        }
        dist1 = spec.build_distribution(params1)
        self.assertEqual(dist1.event_shape, dist.event_shape)
        self.assertEqual(type(dist1), dist_utils.DiagMultivariateNormal)
        self.assertEqual(type(dist1.base_dist), td.Normal)
        self.assertEqual(dist1.base_dist.mean, params1['loc'])
        self.assertEqual(dist1.base_dist.stddev, params1['scale'])

        self.assertRaises(RuntimeError, spec.build_distribution,
                          {'loc': torch.tensor([1., 2.])})

    def test_diag_multivariate_cauchy(self):
        dist = dist_utils.DiagMultivariateCauchy(
            torch.tensor([[1., 2.], [2., 2.]]),
            torch.tensor([[2., 3.], [1., 1.]]))
        spec = dist_utils.DistributionSpec.from_distribution(dist)

        params1 = {
            'loc': torch.tensor([[0.5, 1.5], [1.0, 1.0]]),
            'scale': torch.tensor([[2., 4.], [2., 1.]])
        }
        dist1 = spec.build_distribution(params1)
        self.assertEqual(dist1.event_shape, dist.event_shape)
        self.assertEqual(type(dist1), dist_utils.DiagMultivariateCauchy)
        self.assertEqual(type(dist1.base_dist), dist_utils.StableCauchy)
        self.assertEqual(dist1.base_dist.loc, params1['loc'])
        self.assertEqual(dist1.base_dist.scale, params1['scale'])

        self.assertRaises(RuntimeError, spec.build_distribution,
                          {'loc': torch.tensor([1., 2.])})

    @parameterized.parameters(True, False)
    def test_onehot_categorical_gumbelsoftmax(self, hard_sample):
        logits = torch.randn([2, 3], requires_grad=True)
        dist = dist_utils.OneHotCategoricalGumbelSoftmax(
            hard_sample=hard_sample, tau=0.1, logits=logits)
        spec = dist_utils.DistributionSpec.from_distribution(dist)

        logits1 = torch.zeros([2, 3])
        dist1 = spec.build_distribution({'logits': logits1})
        self.assertEqual(dist1._hard_sample, hard_sample)
        self.assertEqual(dist1._tau, 0.1)
        self.assertTensorEqual(dist1.probs, torch.ones([2, 3]) / 3.)

        sample = dist.rsample()
        self.assertEqual(sample.shape, logits.shape)
        self.assertTrue(sample.requires_grad)
        self.assertTrue(dist.mode.requires_grad)


class TransformationAndInversionTest(parameterized.TestCase,
                                     alf.test.TestCase):
    def test_transformed(self):
        normal_dist = dist_utils.DiagMultivariateNormal(
            torch.tensor([[1., 2.], [2., 2.]]),
            torch.tensor([[2., 3.], [1., 1.]]))
        transforms = [dist_utils.SigmoidTransform()]
        dist = td.TransformedDistribution(
            base_distribution=normal_dist, transforms=transforms)
        spec = dist_utils.DistributionSpec.from_distribution(dist)

        params1 = {
            "transforms_params_": [{}],
            "params_": {
                'loc': torch.tensor([[0.5, 1.5], [1.0, 1.0]]),
                'scale': torch.tensor([[2., 4.], [2., 1.]])
            }
        }
        dist1 = spec.build_distribution(params1)
        self.assertEqual(type(dist1), td.TransformedDistribution)
        self.assertEqual(dist1.event_shape, dist.event_shape)
        self.assertEqual(dist1.transforms, transforms)
        self.assertEqual(
            type(dist1.base_dist), dist_utils.DiagMultivariateNormal)
        self.assertEqual(type(dist1.base_dist.base_dist), td.Normal)
        self.assertEqual(dist1.base_dist.base_dist.mean,
                         params1['params_']['loc'])
        self.assertEqual(dist1.base_dist.base_dist.stddev,
                         params1['params_']['scale'])

    @parameterized.parameters(math_ops.identity, torch.detach, torch.clone)
    def test_inversion(self, func):
        x = torch.tensor([-10.0, -8.6, -2.0, 0, 2, 8.6, 10.0])
        loc = torch.tensor([0.5])
        scale = torch.tensor([1.5])

        # transformation cache is on by using transformations from dist_utils
        forward_transforms = [
            dist_utils.StableTanh(),
            dist_utils.AffineTransform(loc=loc, scale=scale)
        ]

        y = x
        # forward
        for transform in forward_transforms:
            y = transform(y)

        def _get_inverse(y, forward_transforms):
            x_recovered = y
            for transform in reversed(forward_transforms):
                x_recovered = transform.inv(x_recovered)
            return x_recovered

        # inverse
        x_recovered = _get_inverse(func(y), forward_transforms)

        if func is math_ops.identity:
            # there is no additional operations that invalidate the cache
            self.assertTensorEqual(x, x_recovered)
            self.assertTrue(x is x_recovered)
        else:
            # some additional operations on action could invalidate the cache,
            # e.g. clone, detach etc.
            self.assertTrue(x is not x_recovered)
            self.assertTensorNotClose(x, x_recovered)

    def test_affine_transformed(self):
        normal_dist = dist_utils.DiagMultivariateNormal(
            torch.tensor([[1., 2.], [2., 2.]]),
            torch.tensor([[2., 3.], [1., 1.]]))
        dist = dist_utils.AffineTransformedDistribution(
            base_dist=normal_dist, loc=1, scale=2)
        self.assertEqual(dist.entropy(),
                         normal_dist.entropy() + math.log(2) * 2)
        spec = dist_utils.DistributionSpec.from_distribution(dist)

        params1 = {
            'loc': torch.tensor([[0.5, 1.5], [1.0, 1.0]]),
            'scale': torch.tensor([[2., 4.], [2., 1.]])
        }
        dist1 = spec.build_distribution(params1)
        self.assertEqual(type(dist1), dist_utils.AffineTransformedDistribution)
        self.assertEqual(dist1.event_shape, dist.event_shape)
        self.assertEqual(
            type(dist1.base_dist), dist_utils.DiagMultivariateNormal)
        self.assertEqual(type(dist1.base_dist.base_dist), td.Normal)
        self.assertEqual(dist1.base_dist.base_dist.mean, params1['loc'])
        self.assertEqual(dist1.base_dist.base_dist.stddev, params1['scale'])


class TestConversions(alf.test.TestCase):
    def test_conversions(self):
        dists = {
            't':
                torch.tensor([[1., 2., 4.], [3., 3., 1.]]),
            'd':
                dist_utils.DiagMultivariateNormal(
                    torch.tensor([[1., 2.], [2., 2.]]),
                    torch.tensor([[2., 3.], [1., 1.]]))
        }
        params = dist_utils.distributions_to_params(dists)
        dists_spec = dist_utils.extract_spec(dists, from_dim=1)
        self.assertEqual(dists_spec['t'],
                         alf.TensorSpec(shape=(3, ), dtype=torch.float32))
        self.assertEqual(type(dists_spec['d']), dist_utils.DistributionSpec)
        self.assertEqual(len(params), 2)
        self.assertEqual(dists['t'], params['t'])
        self.assertEqual(dists['d'].base_dist.mean, params['d']['loc'])
        self.assertEqual(dists['d'].base_dist.stddev, params['d']['scale'])

        dists1 = dist_utils.params_to_distributions(params, dists_spec)
        self.assertEqual(len(dists1), 2)
        self.assertEqual(dists1['t'], dists['t'])
        self.assertEqual(type(dists1['d']), type(dists['d']))

        params_spec = dist_utils.to_distribution_param_spec(dists_spec)
        alf.nest.assert_same_structure(params_spec, params)
        params1_spec = dist_utils.extract_spec(params)
        self.assertEqual(params_spec, params1_spec)


class TestActionSamplingCategorical(alf.test.TestCase):
    def test_action_sampling_categorical(self):
        m = torch.distributions.categorical.Categorical(
            torch.Tensor([0.25, 0.75]))
        M = m.expand([10])
        epsilon = 0.0
        action_expected = torch.Tensor([1]).repeat(10)
        action_obtained = dist_utils.epsilon_greedy_sample(M, epsilon)
        self.assertTrue((action_expected == action_obtained).all())


class TestActionSamplingNormal(alf.test.TestCase):
    def test_action_sampling_normal(self):
        m = torch.distributions.normal.Normal(
            torch.Tensor([0.3, 0.7]), torch.Tensor([1.0, 1.0]))
        M = m.expand([10, 2])
        epsilon = 0.0
        action_expected = torch.Tensor([0.3, 0.7]).repeat(10, 1)
        action_obtained = dist_utils.epsilon_greedy_sample(M, epsilon)
        self.assertTrue((action_expected == action_obtained).all())


class TestActionSamplingTransformedNormal(alf.test.TestCase):
    def test_action_sampling_transformed_normal(self):
        def _get_transformed_normal(means, stds):
            normal_dist = td.Independent(td.Normal(loc=means, scale=stds), 1)
            transforms = [
                dist_utils.StableTanh(),
                dist_utils.AffineTransform(
                    loc=torch.tensor(0.), scale=torch.tensor(5.0))
            ]
            squashed_dist = td.TransformedDistribution(
                base_distribution=normal_dist, transforms=transforms)
            return squashed_dist, transforms

        means = torch.Tensor([0.3, 0.7])
        dist, transforms = _get_transformed_normal(
            means=means, stds=torch.Tensor([1.0, 1.0]))

        mode = dist_utils.get_mode(dist)

        transformed_mode = means
        for transform in transforms:
            transformed_mode = transform(transformed_mode)

        self.assertTrue((transformed_mode == mode).all())

        epsilon = 0.0
        action_obtained = dist_utils.epsilon_greedy_sample(dist, epsilon)
        self.assertTrue((transformed_mode == action_obtained).all())


class TestActionSamplingTransformedCategorical(alf.test.TestCase):
    def test_action_sampling_transformed_categorical(self):
        def _get_transformed_categorical(probs):
            categorical_dist = td.Independent(td.Categorical(probs=probs), 1)
            return categorical_dist

        probs = torch.Tensor([[0.3, 0.5, 0.2], [0.6, 0.4, 0.0]])
        dist = _get_transformed_categorical(probs=probs)
        mode = dist_utils.get_mode(dist)
        expected_mode = torch.argmax(probs, dim=1)

        self.assertTensorEqual(expected_mode, mode)

        epsilon = 0.0
        action_obtained = dist_utils.epsilon_greedy_sample(dist, epsilon)
        self.assertTensorEqual(expected_mode, action_obtained)


class TestRSampleActionDistribution(alf.test.TestCase):
    def test_rsample_action_distribution(self):
        c = torch.distributions.categorical.Categorical(
            torch.Tensor([0.25, 0.75]))
        C = c.expand([10])
        self.assertRaises(AssertionError,
                          dist_utils.rsample_action_distribution, C)

        n = torch.distributions.normal.Normal(
            torch.Tensor([0.3, 0.7]), torch.Tensor([1.0, 1.0]))
        N = n.expand([10, 2])

        action_distribution = ActionDistribution(a=C, b=N)
        self.assertRaises(AssertionError,
                          dist_utils.rsample_action_distribution,
                          action_distribution)


class TestSoftTransforms(alf.test.TestCase):
    def test_soft_transforms(self):
        N = 100
        x = torch.randn([N, N], dtype=torch.float32, requires_grad=True)
        softplus = dist_utils.Softplus()
        softplus_x = softplus(x)
        softplus_x_ = torch.nn.functional.softplus(x)

        self.assertTrue(torch.all(softplus_x >= 0.))
        self.assertTensorClose(softplus._inverse(softplus_x), x)
        self.assertTensorClose(softplus_x, softplus_x_)

        b = 0.1
        softlower = dist_utils.Softlower(b)
        softlower_x = softlower(x)
        softlower_x_ = math_ops.softlower(x, b)
        self.assertTrue(torch.all(softlower_x >= b))
        self.assertTensorClose(softlower.inv(softlower_x), x)
        self.assertTensorClose(softlower_x, softlower_x_)

        b = -0.01
        softupper = dist_utils.Softupper(b, hinge_softness=0.01)
        softupper_x = softupper(x)
        softupper_x_ = math_ops.softupper(x, b, hinge_softness=0.01)
        self.assertTrue(torch.all(softupper_x <= b))
        self.assertTensorClose(softupper.inv(softupper_x), x)
        self.assertTensorClose(softupper_x, softupper_x_)

        b = 1e-4
        softclip = dist_utils.SoftclipTF(-b, b, hinge_softness=1e-4)
        softclip_x = softclip(x)
        softclip_x_ = math_ops.softclip_tf(x, -b, b, hinge_softness=1e-4)
        self.assertTrue(torch.all(softclip_x <= b))
        self.assertTrue(torch.all(softclip_x >= -b))
        self.assertTensorClose(softclip.inv(softclip_x), x)
        self.assertTensorClose(softclip_x, softclip_x_)

        b = 1e-4
        softclip = dist_utils.Softclip(-b, b, hinge_softness=1e-4)
        softclip_x = softclip(x)
        softclip_x_ = math_ops.softclip(x, -b, b, hinge_softness=1e-4)
        self.assertTrue(torch.all(softclip_x <= b))
        self.assertTrue(torch.all(softclip_x >= -b))
        self.assertTensorClose(softclip.inv(softclip_x), x)
        self.assertTensorClose(softclip_x, softclip_x_)

        # test Softclip._inverse in mild conditions
        b = 1
        softclip = dist_utils.Softclip(-b, b, hinge_softness=1)
        self.assertTensorClose(softclip._inverse(softclip(x)), x, epsilon=1e-4)

        y = softclip(x)
        grad = torch.autograd.grad(y.sum(), x)[0]
        self.assertTensorClose(
            grad.log(), softclip.log_abs_det_jacobian(x, y), epsilon=1e-5)


class TestNFTransformedDistributionParams(alf.test.TestCase):
    def test_distribution_params(self):
        spec = alf.tensor_specs.TensorSpec((4, ))
        scale_trans_net = NetworkWrapper(lambda xz: x + z,
                                         (spec, spec)).make_parallel(2)

        z1 = spec.rand(outer_dims=(1, ))
        z2 = spec.rand(outer_dims=(1, ))
        mask = spec.ones(outer_dims=(1, ))

        t1 = _RealNVPTransform(spec, scale_trans_net, mask, spec, z1)
        t2 = _RealNVPTransform(spec, scale_trans_net, mask, spec, z2)
        t3 = dist_utils.StableTanh()
        t = td.ComposeTransform([t1, t2])
        t_ = td.ComposeTransform([t, t3])

        mean = spec.zeros(outer_dims=(1, ))
        std = spec.ones(outer_dims=(1, ))
        dist = dist_utils.DiagMultivariateNormal(mean, std)
        transformed_dist1 = td.TransformedDistribution(dist, [t])
        transformed_dist2 = td.TransformedDistribution(dist, [t_])
        transformed_dist3 = td.TransformedDistribution(dist, [t1, t2])
        transformed_dist4 = td.TransformedDistribution(transformed_dist1, [t3])

        params1 = dist_utils.distributions_to_params(transformed_dist1)
        z12 = {'parts_params': [{'z': z1}, {'z': z2}]}
        self.assertEqual(params1, {
            'transforms_params_': [z12],
            'params_': {
                'loc': mean,
                'scale': std
            }
        })

        params2 = dist_utils.distributions_to_params(transformed_dist2)
        self.assertEqual(
            params2, {
                'transforms_params_': [{
                    'parts_params': [z12, {}]
                }],
                'params_': {
                    'loc': mean,
                    'scale': std
                }
            })

        params3 = dist_utils.distributions_to_params(transformed_dist3)
        self.assertEqual(params3['transforms_params_'], z12['parts_params'])

        params4 = dist_utils.distributions_to_params(transformed_dist4)
        self.assertEqual(
            params4, {
                'transforms_params_': [{}],
                'params_': {
                    'transforms_params_': [z12],
                    'params_': {
                        'loc': mean,
                        'scale': std
                    }
                }
            })

        dist_spec2 = dist_utils.DistributionSpec.from_distribution(
            transformed_dist2)

        z4 = spec.rand(outer_dims=(1, ))
        z5 = spec.rand(outer_dims=(1, ))
        t4 = _RealNVPTransform(spec, scale_trans_net, mask, spec, z4)
        t5 = _RealNVPTransform(spec, scale_trans_net, mask, spec, z5)
        # similar to transformed_dist2, but with different params
        transformed_dist5 = td.TransformedDistribution(
            dist_utils.DiagMultivariateNormal(
                spec.ones(outer_dims=(1, )), std),
            [td.ComposeTransform([td.ComposeTransform([t4, t5]), t3])])
        params5 = dist_utils.distributions_to_params(transformed_dist5)

        built_dist5 = dist_spec2.build_distribution(params5)

        self.assertEqual(built_dist5.base_dist.mean,
                         spec.ones(outer_dims=(1, )))
        self.assertEqual(built_dist5.transforms[0].parts[0].parts[0].params,
                         {'z': z4})

        dist_spec4 = dist_utils.DistributionSpec.from_distribution(
            transformed_dist4)
        transformed_dist6 = td.TransformedDistribution(
            td.TransformedDistribution(dist, [td.ComposeTransform([t4, t5])]),
            [t3])
        params6 = dist_utils.distributions_to_params(transformed_dist6)

        built_dist6 = dist_spec4.build_distribution(params6)
        self.assertEqual(built_dist6.base_dist.transforms[0].parts[0].params,
                         {'z': z4})

        # we build another distribution using dist_spec4 and check if ``built_dist6``
        # has an unchanged ``z``
        built_dist7 = dist_spec4.build_distribution(
            alf.nest.map_structure(torch.zeros_like, params6))
        self.assertFalse(built_dist7.base_dist.transforms[0] is built_dist6.
                         base_dist.transforms[0])
        self.assertTensorEqual(
            built_dist7.base_dist.transforms[0].parts[0].params['z'],
            torch.zeros_like(z4))
        self.assertEqual(built_dist6.base_dist.transforms[0].parts[0].params,
                         {'z': z4})


if __name__ == '__main__':
    logging.set_verbosity(logging.INFO)
    alf.test.main()
