\documentclass{article}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{mathbbol}

\newcommand{\tr}{\mathrm{tr}}
\newcommand{\erf}{\mathrm{erf}}
\newcommand{\pian}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\vect}[1]{\left[\begin{array}{c} #1 \end{array}\right]}
\newcommand{\vectwo}[2]{\left[ \begin{array}{c c} #1 & #2 \\ \end{array} \right]}
\newcommand{\innerprod}[2]{\left< #1 , #2 \right> }
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
	

\title{Subtleties of estimating entropy}
\author{Wei Xu}
\begin{document}
	\maketitle

For some algorithms, we need to calculate the entropy and its derivative. If there is no analytic formula for the entropy, we can resort to sampling. Given the definition of entropy:
\begin{equation}
H(p) = E_{x\sim p_\theta}(-\log p_\theta(x))
\end{equation}
we can see that $-\log p(x)$ is an unbiased estimator of $H$ if $x$ is sampled from $p$. It is tempting to use $-\pian{\log p_\theta(x)}{\theta}$ as an estimator of $\pian{H}{\theta}$. However, it is wrong, as shown in the following: 
\begin{eqnarray*}
&&	E_{x\sim p_\theta}\left(\pian{\log p_\theta(x)}{\theta}\right) = \int \pian{\log p_\theta(x)}{\theta} p_\theta(x) dx \\
&=& \int \pian{p_\theta(x)}{\theta} dx = \pian{}{\theta} \int p_\theta(x) dx = \pian{1}{\theta} = 0
\end{eqnarray*}

We need to actually go through the process of calculating the derivative to get the unbiased estimator
of $\pian{H}{\theta}$:
\begin{eqnarray*}
\pian{H}{\theta}&=&-\pian{}{\theta}\int \log p_\theta(x) p_\theta(x) dx \\
&=& - \int \left(\pian{\log p_\theta(x)}{\theta}p_\theta(x) + \log p_\theta(x) \pian{p_\theta(x)}{\theta}\right) dx \\
&=& - \int \left(\pian{\log p_\theta(x)}{\theta}p_\theta(x) + \log p_\theta(x) \pian{\log p_\theta(x)}{\theta} p_\theta(x)\right) dx \\
&=& - \int (1+\log p_\theta(x))\pian{\log p_\theta(x)}{\theta} p_\theta(x) dx \\
&=& -E_{x\sim p_\theta}\left(\log p_\theta(x)\pian{\log p_\theta(x)}{\theta}\right) -E_{x\sim p_\theta}\left(\pian{\log p_\theta(x)}{\theta}\right) \\
&=& -\frac{1}{2}E_{x\sim p_\theta}\left(\pian{}{\theta}(\log p_\theta(x))^2\right) \\
\end{eqnarray*}
This means that $-\frac{1}{2}\pian{}{\theta}(\log p_\theta(x))^2$ is an unbiased estimator of $\pian{H}{\theta}$. Actually, $-\frac{1}{2}\pian{}{\theta}(c+\log p_\theta(x))^2$ is an unbiased
estimator for any constant $c$.

For some distributions, the sample of $p_\theta$ is generated by transforming $\epsilon \sim q$ by $f_\theta(\epsilon)$, where $q$ is a fixed distribution and $f_\theta$ is a smooth bijective mapping. $p_\theta(x)$ is implicitly defined by $q$ and $f_\theta$ as:
\[ p_\theta(x) = q(f_\theta^{-1}(x)) / \left|\det \left.\pian{f_\theta(\epsilon)}{\epsilon}\right|_{\epsilon=f_\theta^{-1}(x)}\right|\]

Interestingly, when calculating $-\pian{\log p_\theta(x)}{\theta}$, if we treat $x$ as $x=f_\theta(\epsilon)$, we get an unbiased estimator of $\pian{H}{\theta}$:
\begin{eqnarray*}
&& E_{x\sim p_\theta}\left(-\pian{\log p_\theta(x)}{\theta}\right) = E_{\epsilon \sim q}\left(-\pian{\log p_\theta(f_\theta(\epsilon))}{\theta}\right) \\
&=& -\pian{}{\theta}E_{\epsilon \sim q}\left(\log p_\theta(f_\theta(\epsilon))\right) = -\pian{}{\theta}E_{x \sim p_\theta}\left(\log p_\theta(x)\right) = \pian{}{\theta}H(p)
\end{eqnarray*}
So we can use $-\pian{\log p_\theta(x)}{\theta}$ as an unbiased estimator of $\pian{H(p)}{\theta}$ if $x=f_\theta(\epsilon)$ and we allow gradient to propagate through $x$ to $\theta$.
\end{document}
