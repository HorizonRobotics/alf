{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entropy_estimation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c21LM0LMOcIu",
        "colab_type": "text"
      },
      "source": [
        "# Subtleties of estimating entropy\n",
        "\n",
        "## Wei Xu\n",
        "\n",
        "For some algorithms, we need to calculate the entropy and its derivative. If there is no analytic formula for the entropy, we can resort to sampling. Given the definition of entropy:\n",
        "\n",
        "$H(p) = E_{x\\sim p_\\theta}(-\\log p_\\theta(x))$\n",
        "\n",
        "We can see that $-\\log p_{\\theta}(x)$ is an unbiased estimator of $H$ if $x$ is sampled from $p_{\\theta}$. It is tempting to use $-\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}$ as an estimator of $\\frac{\\partial H}{\\partial\\theta}$. However, it is wrong, as shown in the following: \n",
        "\n",
        "$E_{x\\sim p_\\theta}\\left(\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}\\right) = \\int \\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta} p_\\theta(x) dx = \\int \\frac{\\partial p_\\theta(x)}{\\partial\\theta} dx = \\frac{\\partial}{\\partial\\theta} \\int p_\\theta(x) dx = \\frac{\\partial 1}{\\partial\\theta} = 0$\n",
        "\n",
        "We need to actually go through the process of calculating the derivative to get the unbiased estimator\n",
        "of $\\frac{\\partial H}{\\partial\\theta}$:\n",
        "\n",
        "\\begin{array}{rll}\n",
        "\\frac{\\partial H}{\\partial\\theta}&=&-\\frac{\\partial}{\\partial\\theta}\\int \\log p_\\theta(x) p_\\theta(x) dx \\\\\n",
        "&=& - \\int \\left(\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}p_\\theta(x) + \\log p_\\theta(x) \\frac{\\partial p_\\theta(x)}{\\partial\\theta}\\right) dx \\\\\n",
        "&=& - \\int \\left(\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}p_\\theta(x) + \\log p_\\theta(x) \\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta} p_\\theta(x)\\right) dx \\\\\n",
        "&=& - \\int (1+\\log p_\\theta(x))\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta} p_\\theta(x) dx \\\\\n",
        "&=& -E_{x\\sim p_\\theta}\\left(\\log p_\\theta(x)\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}\\right) -E_{x\\sim p_\\theta}\\left(\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}\\right) \\\\\n",
        "&=& -\\frac{1}{2}E_{x\\sim p_\\theta}\\left(\\frac{\\partial}{\\partial\\theta}(\\log p_\\theta(x))^2\\right) \\\\\n",
        "\\end{array}\n",
        "\n",
        "This means that $-\\frac{1}{2}\\frac{\\partial}{\\partial\\theta}(\\log p_\\theta(x))^2$ is an unbiased estimator of $\\frac{\\partial H}{\\partial\\theta}$. Actually, $-\\frac{1}{2}\\frac{\\partial}{\\partial\\theta}(c+\\log p_\\theta(x))^2$ is an unbiased estimator for any constant $c$.\n",
        "\n",
        "For some distributions, the sample of $p_\\theta$ is generated by transforming $\\epsilon \\sim q$ by $f_\\theta(\\epsilon)$, where $q$ is a fixed distribution and $f_\\theta$ is a smooth bijective mapping. $p_\\theta(x)$ is implicitly defined by $q$ and $f_\\theta$ as:\n",
        "\n",
        "$p_\\theta(x) = q(f_\\theta^{-1}(x)) / \\left|\\det \\left.\\frac{\\partial f_\\theta(\\epsilon)}{\\partial\\epsilon}\\right|_{\\epsilon=f_\\theta^{-1}(x)}\\right|$\n",
        "\n",
        "Interestingly, when calculating $-\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}$, if we treat $x$ as $x=f_\\theta(\\epsilon)$, we get an unbiased estimator of $\\frac{\\partial H}{\\partial\\theta}$:\n",
        "\n",
        "\\begin{array}{rll}\n",
        "&& E_{x\\sim p_\\theta}\\left(-\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}\\right) = E_{\\epsilon \\sim q}\\left(-\\frac{\\partial\\log p_\\theta(f_\\theta(\\epsilon))}{\\partial\\theta}\\right) \\\\\n",
        "&=& -\\frac{\\partial}{\\partial\\theta}E_{\\epsilon \\sim q}\\left(\\log p_\\theta(f_\\theta(\\epsilon))\\right) = -\\frac{\\partial}{\\partial\\theta}E_{x \\sim p_\\theta}\\left(\\log p_\\theta(x)\\right) = \\frac{\\partial}{\\partial\\theta}H(p)\n",
        "\\end{array}\n",
        "\n",
        "So we can use $-\\frac{\\partial\\log p_\\theta(x)}{\\partial\\theta}$ as an unbiased estimator of $\\frac{\\partial H(p)}{\\partial\\theta}$ if $x=f_\\theta(\\epsilon)$ and we allow gradient to propagate through $x$ to $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iFyul462QURL",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}