import alf.algorithms.agent
import alf.algorithms.actor_critic_loss
import alf.algorithms.icm_algorithm
import alf.algorithms.entropy_target_algorithm


# environment config
create_environment.num_parallel_environments=30
create_environment.env_name='MountainCar-v0'

# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.action_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(256, 256)

value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(256, 256)

ac/Adam.lr=1e-3
feature_size=200
icm/encoding_net_fc_layer_params=(%feature_size, %feature_size)
icm/EncodingNetwork.input_tensor_spec=%observation_spec
icm/EncodingNetwork.fc_layer_params=%icm/encoding_net_fc_layer_params
icm/EncodingNetwork.activation=@torch.relu

ICMAlgorithm.action_spec=%action_spec
ICMAlgorithm.activation=@torch.relu
ICMAlgorithm.encoding_net=@icm/EncodingNetwork()
ICMAlgorithm.hidden_size=(%feature_size, %feature_size)

ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
Agent.optimizer=@ac/Adam()
Agent.intrinsic_reward_module=@ICMAlgorithm()
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

Agent.enforce_entropy_target=True
EntropyTargetAlgorithm.min_alpha=0.1

# training config
TrainerConfig.unroll_length=100
TrainerConfig.algorithm_ctor=@Agent
TrainerConfig.num_iterations=1000000
TrainerConfig.debug_summaries=1
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.summary_interval=20
