import alf.algorithms.actor_critic_algorithm
import alf.trainers.on_policy_trainer

# environment config
create_environment.num_parallel_environments=30
create_environment.env_name='MountainCar-v0'
create_environment.env_load_fn=@load_with_random_max_episode_steps
load_with_random_max_episode_steps.min_steps=1000
load_with_random_max_episode_steps.max_steps=1200
suite_gym.load.max_episode_steps=1000

# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.output_tensor_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(256,)
actor/ActorDistributionNetwork.activation_fn=@tf.nn.elu

value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(256,)
value/ValueNetwork.activation_fn=@tf.nn.elu

ac/Adam.learning_rate=5e-5

icm/EncodingNetwork.input_tensor_spec=%observation_spec
icm/EncodingNetwork.fc_layer_params=(200,)
icm/EncodingNetwork.activation_fn=@tf.nn.elu
icm/tf.TensorSpec.shape=(200,)

ICMAlgorithm.action_spec=%action_spec
ICMAlgorithm.feature_spec=@icm/tf.TensorSpec()
ICMAlgorithm.encoding_net=@icm/EncodingNetwork()
ICMAlgorithm.hidden_size=200

ActorCriticAlgorithm.action_spec=%action_spec
ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
ActorCriticAlgorithm.optimizer=@ac/Adam()
ActorCriticAlgorithm.intrinsic_curiosity_module=@ICMAlgorithm()
ActorCriticLoss.entropy_regularization=0.1

# training config
TrainerConfig.trainer=@on_policy_trainer
TrainerConfig.unroll_length=100
TrainerConfig.algorithm_ctor=@ActorCriticAlgorithm
TrainerConfig.num_iterations=1000000
TrainerConfig.use_tf_functions=1
TrainerConfig.debug_summaries=1
TrainerConfig.summarize_grads_and_vars=1


