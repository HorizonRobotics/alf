import alf.algorithms.actor_critic_algorithm
import alf.trainers.on_policy_trainer

# environment config
create_environment.num_parallel_environments=30
create_environment.env_name='MountainCar-v0'
create_environment.env_load_fn=@load_with_random_max_episode_steps
load_with_random_max_episode_steps.min_steps=1000
load_with_random_max_episode_steps.max_steps=1200
suite_gym.load.max_episode_steps=1000

# algorithm config

ActorCriticLoss.entropy_regularization=0.1
ActorDistributionNetwork.activation_fn=@tf.nn.elu
ValueNetwork.activation_fn=@tf.nn.elu

create_ac_algorithm.use_icm=1
create_ac_algorithm.actor_fc_layers=(256,)
create_ac_algorithm.value_fc_layers=(256,)
create_ac_algorithm.encoding_fc_layers=(200,)

ICMAlgorithm.hidden_size=200
EncodingNetwork.activation_fn=@tf.nn.elu

# training config
OnPolicyTrainer.train_interval=100
Trainer.algorithm_ctor=@create_ac_algorithm
Trainer.num_iterations=1000000
Trainer.use_tf_functions=1
Trainer.debug_summaries=1
Trainer.summarize_grads_and_vars=1

train_eval.trainer_cls=@OnPolicyTrainer
