import alf.algorithms.actor_critic_algorithm


# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
RLAlgorithm.reward_shaping_fn = @reward_scaling
RLAlgorithm.gradient_clipping=10.0
common.reward_scaling.scale = 0.01

# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.action_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(100,)

value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(100,)

ac/Adam.lr=1e-3

ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
ActorCriticAlgorithm.optimizer=@ac/Adam()

import alf.algorithms.trac_algorithm
ActorCriticLoss.entropy_regularization=1e-4
ActorCriticLoss.gamma=0.98
ActorCriticLoss.td_error_loss_fn=@element_wise_huber_loss
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

# training config
TrainerConfig.unroll_length=10
TrainerConfig.algorithm_ctor=@TracAlgorithm
TrainerConfig.num_iterations=2500
TrainerConfig.num_checkpoints=5
TrainerConfig.evaluate=True
TrainerConfig.eval_interval=500
TrainerConfig.debug_summaries=False
TrainerConfig.summarize_grads_and_vars=False
TrainerConfig.summary_interval=5

# TODO: Fix alf.utils.dist_utils.epsilon_greedy_sample() to handle
# epsilon_greedy < 1.0 and change the default to 0.1
TrainerConfig.epsilon_greedy=1.0