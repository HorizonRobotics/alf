import alf.algorithms.actor_critic_algorithm
import alf.trainers.on_policy_trainer

# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
ActorCriticAlgorithm.reward_shaping_fn = @reward_scaling
common.reward_scaling.scale = 0.01

# algorithm config
ActorCriticAlgorithm.gradient_clipping=10.0
ActorCriticLoss.entropy_regularization=1e-4
ActorCriticLoss.gamma=0.98
ActorCriticLoss.td_error_loss_fn=@element_wise_huber_loss
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True
create_ac_algorithm.actor_fc_layers=(100,)
create_ac_algorithm.value_fc_layers=(100,)
create_ac_algorithm.learning_rate=0.001

# training config
TrainerConfig.trainer=@on_policy_trainer
TrainerConfig.unroll_length=8
TrainerConfig.algorithm_ctor=@create_ac_algorithm
TrainerConfig.num_iterations=20
TrainerConfig.checkpoint_interval=3
TrainerConfig.evaluate=False
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=False
TrainerConfig.summary_interval=5


