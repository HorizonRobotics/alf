import alf.algorithms.actor_critic_algorithm


# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
TrainerConfig.data_transformer_ctor = @RewardScaling
RewardScaling.scale = 0.01

# algorithm config

actor/ActorDistributionNetwork.fc_layer_params=(100,)
value/ValueNetwork.fc_layer_params=(100,)

ac/Adam.lr=1e-3
ac/Adam.gradient_clipping=10.0

ActorCriticAlgorithm.actor_network_ctor=@actor/ActorDistributionNetwork
ActorCriticAlgorithm.value_network_ctor=@value/ValueNetwork
ActorCriticAlgorithm.optimizer=@ac/Adam()

import alf.algorithms.trac_algorithm
ActorCriticLoss.entropy_regularization=1e-4
ActorCriticLoss.gamma=0.98
ActorCriticLoss.td_error_loss_fn=@element_wise_huber_loss
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

# training config
TrainerConfig.unroll_length=10
TrainerConfig.algorithm_ctor=@TracAlgorithm
TrainerConfig.num_iterations=2500
TrainerConfig.num_checkpoints=5
TrainerConfig.evaluate=True
TrainerConfig.eval_interval=500
TrainerConfig.debug_summaries=False
TrainerConfig.summarize_grads_and_vars=False
TrainerConfig.summary_interval=5
TrainerConfig.epsilon_greedy=0.1