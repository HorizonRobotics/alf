# Warning: Directly converted from the TF version, this gin file was NOT TESTED.

include 'ppo.gin'
import social_bot
import alf.environments.suite_socialbot

# environment config
create_environment.env_name="SocialBot-PlayGround-v0"
create_environment.num_parallel_environments=3 # 16
create_environment.env_load_fn=@suite_socialbot.load
# misc
GoalTask.move_goal_during_episode=False
GoalTask.success_with_angle_requirement=False

# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorDistributionNetwork.fc_layer_params=(128, 64)
actor/ActorDistributionNetwork.activation_fn=@torch.tanh
actor/ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork
NormalProjectionNetwork.init_means_output_factor=1e-10
NormalProjectionNetwork.std_bias_initializer_value=0.0

value/ValueNetwork.fc_layer_params=(128, 64)
value/ValueNetwork.activation_fn=@torch.tanh

ac/Adam.learning_rate=3e-4

# icm
icm/encoding_net_fc_layer_params=(128,)
icm/EncodingNetwork.input_tensor_spec=%observation_spec
icm/EncodingNetwork.fc_layer_params=%icm/encoding_net_fc_layer_params
icm/EncodingNetwork.activation_fn=@torch.nn.functional.elu_
icm/TensorSpec.shape=%icm/encoding_net_fc_layer_params
ICMAlgorithm.action_spec=%action_spec
ICMAlgorithm.feature_spec=@icm/TensorSpec()
ICMAlgorithm.encoding_net=@icm/EncodingNetwork()
ICMAlgorithm.hidden_size=128
Agent.intrinsic_reward_module=@ICMAlgorithm()
Agent.extrinsic_reward_coef=0.0
Agent.intrinsic_reward_coef=1.0
# #


# ActorCriticAlgorithm
ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
Agent.optimizer=@ac/Adam()
Agent.gradient_clipping=0.5
Agent.clip_by_global_norm=True


PPOLoss.entropy_regularization=5e-3
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss


TrainerConfig.mini_batch_length=1
TrainerConfig.unroll_length=1024
TrainerConfig.mini_batch_size=4096
TrainerConfig.num_iterations=500
TrainerConfig.summary_interval=1
TrainerConfig.num_updates_per_train_iter=20
TrainerConfig.eval_interval=100
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.debug_summaries=True
TrainerConfig.checkpoint_interval=1
TrainerConfig.use_rollout_state=True
TrainerConfig.use_tf_functions = True
TrainerConfig.random_seed = None

ReplayBuffer.max_length=2048
