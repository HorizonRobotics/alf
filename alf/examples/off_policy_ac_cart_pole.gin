# Although this config uses off_policy_trainer, it is essentially same as
# on-policy training because the collected experiences are used once (
# OffPolicyTrainer.num_updates_per_train_step=1) for updating policy.
# It is used to validate the correctness of off_policy_trainer.
#
# And in order to do correct off-policy AC, the loss need to includes importance
# ratio, such as the V-trace in IMPALA (arXiv:1802.01561)

import alf.algorithms.actor_critic_algorithm
import alf.trainers.off_policy_trainer

# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
ActorCriticAlgorithm.reward_shaping_fn = @reward_scaling
common.reward_scaling.scale = 0.01

# algorithm config

ActorCriticAlgorithm.gradient_clipping=10.0
ActorCriticLoss.entropy_regularization=1e-4
ActorCriticLoss.gamma=0.98
ActorCriticLoss.td_error_loss_fn=@element_wise_huber_loss
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True
create_ac_algorithm.actor_fc_layers=(100,)
create_ac_algorithm.value_fc_layers=(100,)
create_ac_algorithm.learning_rate=0.001
create_ac_algorithm.off_policy=True

# training config
OffPolicyTrainer.mini_batch_length=9
OffPolicyTrainer.unroll_length=9
OffPolicyTrainer.mini_batch_size=8
OffPolicyTrainer.num_updates_per_train_step=1
Trainer.algorithm_ctor=@create_ac_algorithm
Trainer.num_iterations=3000
Trainer.checkpoint_interval=100000
Trainer.evaluate=False
Trainer.debug_summaries=True
Trainer.summary_interval=5

TFUniformReplayBuffer.max_length=2048

train_eval.trainer_cls=@SyncOffPolicyTrainer
