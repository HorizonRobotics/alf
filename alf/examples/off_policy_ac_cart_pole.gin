# Although this config uses off_policy_trainer, it is essentially same as
# on-policy training because the collected experiences are used once (
# TrainerConfig.num_updates_per_train_iter=1) for updating policy.
# It is used to validate the correctness of off_policy_trainer.
#
# And in order to do correct off-policy AC, the loss need to includes importance
# ratio, such as the V-trace in IMPALA (arXiv:1802.01561)

import alf.algorithms.actor_critic_algorithm
import alf.trainers.off_policy_trainer

# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
TrainerConfig.data_transformer_ctor = @RewardScaling
RewardScaling.scale = 0.01

# algorithm config

observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.output_tensor_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(100,)

value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(100,)

ac/Adam.learning_rate=1e-3

ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
Agent.optimizer=@ac/Adam()

Agent.gradient_clipping=10.0
ActorCriticLoss.entropy_regularization=1e-4
ActorCriticLoss.gamma=0.98
ActorCriticLoss.td_error_loss_fn=@element_wise_huber_loss
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

# training config
TrainerConfig.trainer=@sync_off_policy_trainer
TrainerConfig.mini_batch_length=9
TrainerConfig.unroll_length=9
TrainerConfig.mini_batch_size=8
TrainerConfig.num_updates_per_train_iter=1
TrainerConfig.algorithm_ctor=@Agent
TrainerConfig.num_iterations=3000
TrainerConfig.checkpoint_interval=100000
TrainerConfig.evaluate=False
TrainerConfig.debug_summaries=True
TrainerConfig.summary_interval=5

ReplayBuffer.max_length=2048


