include 'ppo.gin'

import alf.environments.suite_socialbot

# environment config

create_environment.env_name='SocialBot-ICubWalkPID-v0'
create_environment.env_load_fn=@suite_socialbot.load
create_environment.num_parallel_environments=32
suite_gym.wrap_env.clip_action=False

# algorithm config
Agent.enforce_entropy_target=True
# EntropyTargetAlgorithm will use calc_default_target_entropy to calculate
# the target entropy.
calc_default_target_entropy.min_prob=0.01
EntropyTargetAlgorithm.skip_free_stage=True
EntropyTargetAlgorithm.slow_update_rate=4e-4
EntropyTargetAlgorithm.fast_update_rate=4e-4
EntropyTargetAlgorithm.initial_alpha=0.05

PPOLoss.entropy_regularization=None
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss

# debug
PPOLoss.check_numerics=True
estimated_entropy.check_numerics=True

actor/ActorDistributionNetwork.fc_layer_params=(256, 128)
actor/ActorDistributionNetwork.activation=@torch.tanh
actor/ActorDistributionNetwork.continuous_projection_net_ctor=@StableNormalProjectionNetwork

StableNormalProjectionNetwork.projection_output_init_gain=1e-5
StableNormalProjectionNetwork.inverse_std_transform='softplus'
StableNormalProjectionNetwork.scale_distribution=True
StableNormalProjectionNetwork.state_dependent_std=True
StableNormalProjectionNetwork.init_std=1.0

value/ValueNetwork.fc_layer_params=(256, 128)
value/ValueNetwork.activation=@torch.tanh

ac/AdamTF.lr=1e-4
ac/AdamTF.gradient_clipping=0.5
ac/AdamTF.clip_by_global_norm=True

ActorCriticAlgorithm.actor_network_ctor=@actor/ActorDistributionNetwork
ActorCriticAlgorithm.value_network_ctor=@value/ValueNetwork
Agent.optimizer=@ac/AdamTF()

# training config
TrainerConfig.debug_summaries=True
TrainerConfig.eval_interval=100
TrainerConfig.evaluate=True
TrainerConfig.mini_batch_length=1
TrainerConfig.mini_batch_size=4096
TrainerConfig.num_updates_per_train_iter=20
TrainerConfig.num_iterations=1000
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.summary_interval=1
TrainerConfig.unroll_length=512
TrainerConfig.summarize_action_distributions=True
