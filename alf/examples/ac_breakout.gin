# You need to install following packages
# pip3 install atari-py opencv-python

# environment config
import tf_agents.environments.suite_atari
import alf.environments.wrappers
# From OpenAI gym wiki:
# "v0 vs v4: v0 has repeat_action_probability of 0.25
#  (meaning 25% of the time the previous action will be used instead of the new action),
#   while v4 has 0 (always follow your issued action)
# Because we already implements frame_skip in AtariPreprocessing, we should always
# use 'NoFrameSkip' Atari environments from OpenAI gym
create_environment.env_name='BreakoutNoFrameskip-v4'
create_environment.env_load_fn=@suite_atari.load
create_environment.num_parallel_environments=64

suite_atari.load.gym_env_wrappers=(@DMAtariPreprocessing,
                                   @FrameStack)
FrameStack.stack_size=4
DMAtariPreprocessing.frame_skip=4
DMAtariPreprocessing.terminal_on_life_loss=True

# algorithm config
ActorCriticLoss.entropy_regularization=0.01
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True
ActorCriticLoss.td_lambda=0.95
ActorCriticLoss.td_loss_weight=0.5
ActorCriticLoss.advantage_clip=None

ActorDistributionNetwork.activation_fn=@tf.nn.relu

CONV_LAYER_PARAMS=((32, 8, 4), (64, 4, 2), (64, 3, 1))
ActorDistributionNetwork.conv_layer_params=%CONV_LAYER_PARAMS
CategoricalProjectionNetwork.logits_init_output_factor=1e-10
ActorDistributionNetwork.discrete_projection_net=@CategoricalProjectionNetwork

ValueNetwork.activation_fn=@tf.nn.relu
ValueNetwork.conv_layer_params=%CONV_LAYER_PARAMS

create_ac_algorithm.actor_fc_layers=(512,)
create_ac_algorithm.value_fc_layers=(512,)

ActorCriticAlgorithm.gradient_clipping=None
ActorCriticAlgorithm.reward_shaping_fn=@reward_clipping
common.reward_clipping.minmax=(-1, 1)
create_ac_algorithm.learning_rate=1e-3

# training config
on_policy_trainer.train.num_iterations=1000000
on_policy_trainer.train.summarize_grads_and_vars=1
on_policy_trainer.train.summary_interval=10
on_policy_trainer.train.train_interval=8
on_policy_trainer.train.use_tf_functions=True

PolicyDriver.observation_transformer=@image_scale_transformer
common.image_scale_transformer.min=0.0
train_eval.debug_summaries=1
train_eval.evaluate=False
