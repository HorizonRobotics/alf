import alf.algorithms.actor_critic_algorithm
import alf.trainers.on_policy_trainer
import alf.environments.suite_socialbot

# environment config
import alf.environments.wrappers
create_environment.env_name='SocialBot-SimpleNavigationDiscreteAction-v0'
create_environment.env_load_fn=@suite_socialbot.load
suite_socialbot.load.gym_env_wrappers=(@FrameStack,)
create_environment.num_parallel_environments=30
SimpleNavigation.resized_image_size=(84, 84)

# algorithm config
ActorCriticLoss.entropy_regularization=0.002
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.output_tensor_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(256,)
actor/ActorDistributionNetwork.activation_fn=@tf.nn.elu
actor/ActorDistributionNetwork.conv_layer_params=((16, 3, 2), (32, 3, 2))
CategoricalProjectionNetwork.logits_init_output_factor=1e-10
actor/ActorDistributionNetwork.discrete_projection_net=@CategoricalProjectionNetwork

value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(256,)
value/ValueNetwork.activation_fn=@tf.nn.elu
value/ValueNetwork.conv_layer_params=((16, 3, 2), (32, 3, 2))

ac/Adam.learning_rate=1e-4

ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
Agent.optimizer=@ac/Adam()
Agent.gradient_clipping=None


# training config
TrainerConfig.trainer=@on_policy_trainer
TrainerConfig.unroll_length=100
TrainerConfig.algorithm_ctor=@Agent
TrainerConfig.num_iterations=1000000
TrainerConfig.use_tf_functions=1
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=1
TrainerConfig.summary_interval=1

Agent.observation_transformer=@image_scale_transformer


