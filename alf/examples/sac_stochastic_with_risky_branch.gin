include 'sac_breakout.gin'

import alf.environments.suite_simple
create_environment.env_name="StochasticWithRiskyBranch"
create_environment.env_load_fn=@suite_simple.load
create_environment.num_parallel_environments=32

TrainerConfig.data_transformer_ctor=None

# override algorithm and training config
AdamTF.lr=1e-3

CONV_LAYER_PARAMS=None
critic/QNetwork.conv_layer_params=%CONV_LAYER_PARAMS
critic/QNetwork.fc_layer_params=(10,)

SacAlgorithm.target_update_period=10

gamma=0.99
OneStepTDLoss.gamma=%gamma
ReplayBuffer.gamma=%gamma

# training config
TrainerConfig.mini_batch_size=512
TrainerConfig.mini_batch_length=2
TrainerConfig.num_updates_per_train_iter=10
TrainerConfig.unroll_length=10
TrainerConfig.epsilon_greedy=0.05
TrainerConfig.initial_collect_steps=500
TrainerConfig.num_iterations=8000
TrainerConfig.num_env_steps=0
TrainerConfig.evaluate=False
TrainerConfig.num_checkpoints=1
TrainerConfig.debug_summaries=True
TrainerConfig.replay_buffer_length=200
