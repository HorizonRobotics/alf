import alf.algorithms.ppo_algorithm
import alf.algorithms.ppo_loss

ActorCriticAlgorithm.loss_class = @PPOLoss

# environment config

# baseline ppo2 training command:
# CUDA_VISIBLE_DEVICES=0 OPENAI_LOGDIR=/home/weixu/tmp/bullet_humanoid_baseline/ppo-0 OPENAI_LOG_FORMAT='stdout,tensorboard' python -m baselines.run --alg=ppo2 --env=HumanoidBulletEnv-v0 --network=mlp --num_timesteps=1e8 --ent_coef=0.01 --num_hidden=32 --num_layers=3 --value_network=copy

# need to `pip install pybullet`
import pybullet_envs
create_environment.env_name="HumanoidBulletEnv-v0"
create_environment.num_parallel_environments=32
ParallelPyEnvironment.start_serially=False

# algorithm config
ActorCriticAlgorithm.gradient_clipping=0.5
ActorCriticAlgorithm.clip_by_global_norm=True

PPOLoss.entropy_regularization=1e-2
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss

ActorDistributionNetwork.activation_fn=@tf.nn.tanh
ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork
NormalProjectionNetwork.init_means_output_factor=1e-10
NormalProjectionNetwork.std_bias_initializer_value=0.0
NormalProjectionNetwork.std_transform=@tf.math.exp
ValueNetwork.activation_fn=@tf.nn.tanh

create_ac_algorithm.actor_fc_layers=(32, 32, 32)
create_ac_algorithm.value_fc_layers=(32, 32, 32)
create_ac_algorithm.learning_rate=3e-4

async_off_policy_trainer.train.use_tf_functions = True
async_off_policy_trainer.train.summary_interval = 10

# driver config
N = 2
OffPolicyAsyncDriver.num_envs = %N
OffPolicyAsyncDriver.num_act_queues = %N
OffPolicyAsyncDriver.act_queue_cap = 1
OffPolicyAsyncDriver.unroll_length = 512
OffPolicyAsyncDriver.learn_queue_cap = 1
OffPolicyAsyncDriver.summarize_grads_and_vars = True
OffPolicyAsyncDriver.exp_replayer_class = @OnetimeExperienceReplayer
OffPolicyAsyncDriver.num_updates_per_train_step = 20
OffPolicyAsyncDriver.mini_batch_length = 1
OffPolicyAsyncDriver.mini_batch_size = 4096

__main__.train.algorithm_ctor = @create_ppo_algorithm
__main__.train.debug_summaries=True
