# icm on SuperMario

import alf.algorithms.agent
import alf.algorithms.actor_critic_algorithm
import alf.algorithms.icm_algorithm
import alf.trainers.policy_trainer
import alf.environments.suite_mario


# environment config
# gym-retro>=0.7.0 is required for this experiment and also
#  a suitable `SuperMarioBros-Nes` rom should be obtain and imported (roms are not included in gym-retro)
#   see `https://retro.readthedocs.io/en/latest/getting_started.html#importing-roms` to import roms
create_environment.env_load_fn=@suite_mario.load
create_environment.env_name="SuperMarioBros-Nes"
create_environment.num_parallel_environments=30
suite_mario.load.state="Level1-1"

# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

ImageEncodingNetwork.same_padding=True

CONV_LAYER_PARAMS=((32,8,4),(64,4,2),(64,3,1))

actor/ActorDistributionNetwork.fc_layer_params=(256,)
actor/ActorDistributionNetwork.activation=@torch.nn.functional.elu
actor/ActorDistributionNetwork.conv_layer_params=%CONV_LAYER_PARAMS
CategoricalProjectionNetwork.logits_init_output_factor=1e-10

value/ValueNetwork.fc_layer_params=(256,256)
value/ValueNetwork.conv_layer_params=%CONV_LAYER_PARAMS
value/ValueNetwork.activation=@torch.nn.functional.elu

ac/AdamTF.lr=3e-5
ac/AdamTF.gradient_clipping=10.0

icm/encoding_net_fc_layer_params=(256,)
icm/EncodingNetwork.input_tensor_spec=%observation_spec
icm/EncodingNetwork.conv_layer_params=%CONV_LAYER_PARAMS
icm/EncodingNetwork.activation=@torch.nn.functional.elu
icm/EncodingNetwork.fc_layer_params=%icm/encoding_net_fc_layer_params

ICMAlgorithm.action_spec=%action_spec
ICMAlgorithm.encoding_net=@icm/EncodingNetwork()
ICMAlgorithm.hidden_size=256
ICMAlgorithm.activation=@torch.nn.functional.elu

ActorCriticAlgorithm.actor_network_ctor=@actor/ActorDistributionNetwork
ActorCriticAlgorithm.value_network_ctor=@value/ValueNetwork
Agent.optimizer=@ac/AdamTF()
Agent.intrinsic_reward_module=@ICMAlgorithm()
TrainerConfig.data_transformer_ctor=@ImageScaleTransformer
ActorCriticLoss.entropy_regularization=0.01
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

# training config
TrainerConfig.unroll_length=32
TrainerConfig.algorithm_ctor=@Agent
TrainerConfig.num_iterations=10000000
TrainerConfig.debug_summaries=1
TrainerConfig.summarize_grads_and_vars=1
TrainerConfig.summary_interval=100
TrainerConfig.use_rollout_state=True
