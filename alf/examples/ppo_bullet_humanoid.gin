include 'ppo.gin'

# environment config

# baseline ppo2 training command:
# CUDA_VISIBLE_DEVICES=0 OPENAI_LOGDIR=/home/weixu/tmp/bullet_humanoid_baseline/ppo-0 OPENAI_LOG_FORMAT='stdout,tensorboard' python -m baselines.run --alg=ppo2 --env=HumanoidBulletEnv-v0 --network=mlp --num_timesteps=1e8 --ent_coef=0.01 --num_hidden=32 --num_layers=3 --value_network=copy

# need to `pip install pybullet`
import pybullet_envs
create_environment.env_name="HumanoidBulletEnv-v0"
create_environment.num_parallel_environments=32

# algorithm config
ActorCriticAlgorithm.gradient_clipping=0.5
ActorCriticAlgorithm.clip_by_global_norm=True

PPOLoss.entropy_regularization=1e-2
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss

ActorDistributionNetwork.activation_fn=@tf.nn.tanh
ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork
NormalProjectionNetwork.init_means_output_factor=1e-10
NormalProjectionNetwork.std_bias_initializer_value=0.0
NormalProjectionNetwork.std_transform=@tf.math.exp
NormalProjectionNetwork.scale_distribution=True
ValueNetwork.activation_fn=@tf.nn.tanh

create_ac_algorithm.actor_fc_layers=(32, 32, 32)
create_ac_algorithm.value_fc_layers=(32, 32, 32)
create_ac_algorithm.learning_rate=3e-4

# training config
off_policy_trainer.train.num_iterations = 100000
off_policy_trainer.train.summarize_grads_and_vars = True
off_policy_trainer.train.num_updates_per_train_step = 20
off_policy_trainer.train.unroll_length = 512
off_policy_trainer.train.mini_batch_length = 1
off_policy_trainer.train.eval_interval = 100
off_policy_trainer.train.summary_interval = 10
off_policy_trainer.train.mini_batch_size = 4096

TFUniformReplayBuffer.max_length = 2048

train_eval.debug_summaries=True
train_eval.evaluate=True
