include 'sac.gin'
include 'carla.gin'

import alf.algorithms.td_loss
import alf.algorithms.encoding_algorithm

encoding_dim = 256
fc_layers_params = (256,)
activation = torch.relu_

actor/StableNormalProjectionNetwork.state_dependent_std=True
actor/StableNormalProjectionNetwork.squash_mean=False
actor/StableNormalProjectionNetwork.scale_distribution=True
actor/StableNormalProjectionNetwork.min_std=1e-3
actor/StableNormalProjectionNetwork.max_std=10.
actor/ActorDistributionNetwork.fc_layer_params=%fc_layers_params
actor/ActorDistributionNetwork.activation=%activation
actor/ActorDistributionNetwork.use_fc_bn=%use_batch_normalization
actor/ActorDistributionNetwork.continuous_projection_net_ctor=@actor/StableNormalProjectionNetwork

critic/CriticNetwork.joint_fc_layer_params=%fc_layers_params
critic/CriticNetwork.activation=%activation
critic/CriticNetwork.use_fc_bn=%use_batch_normalization
critic/CriticNetwork.output_tensor_spec=@TensorSpec((%CarlaEnvironment.REWARD_DIMENSION,))

Adam.lr=1e-4

SacAlgorithm.actor_network_cls=@actor/ActorDistributionNetwork
SacAlgorithm.critic_network_cls=@critic/CriticNetwork
SacAlgorithm.actor_optimizer=@Adam()
SacAlgorithm.critic_optimizer=@Adam()
SacAlgorithm.alpha_optimizer=@Adam()
sac/calc_default_target_entropy.min_prob=0.1
SacAlgorithm.target_entropy=@sac/calc_default_target_entropy
SacAlgorithm.target_update_tau=0.005
SacAlgorithm.critic_loss_ctor=@TDLoss
SacAlgorithm.use_parallel_network=True
SacAlgorithm.use_entropy_reward=False
SacAlgorithm.reward_weights=[1, 0., 0., 0.]

# config EncodingAlgorithm
encoder/EncodingNetwork.input_preprocessors=%input_preprocessors
encoder/EncodingNetwork.preprocessing_combiner=@NestSum(activation=%activation, average=True)
encoder/EncodingNetwork.activation=%activation
encoder/EncodingNetwork.fc_layer_params=%fc_layers_params
encoder/EncodingAlgorithm.encoder_cls=@encoder/EncodingNetwork

# config PredictiveRepresentationLearner
import alf.algorithms.predictive_representation_learner

PredictiveRepresentationLearner.num_unroll_steps=10
decoder/EncodingNetwork.fc_layer_params=%fc_layers_params
decoder/EncodingNetwork.last_layer_size=%CarlaEnvironment.REWARD_DIMENSION
decoder/EncodingNetwork.last_activation=@identity
decoder/EncodingNetwork.last_kernel_initializer=torch.nn.init.zeros_
SimpleDecoder.decoder_net_ctor=@decoder/EncodingNetwork
SimpleDecoder.target_field='reward'
SimpleDecoder.summarize_each_dimension=True
PredictiveRepresentationLearner.decoder_ctor=@SimpleDecoder
PredictiveRepresentationLearner.encoding_net_ctor=@encoder/EncodingNetwork
dynamics/LSTMEncodingNetwork.preprocessing_combiner=@NestSum(activation=%activation, average=True)
dynamics/LSTMEncodingNetwork.hidden_size=(%encoding_dim, %encoding_dim)
PredictiveRepresentationLearner.dynamics_net_ctor=@dynamics/LSTMEncodingNetwork
ReplayBuffer.keep_episodic_info=True

# Change to `ReprLearner=@encoder/EncodingAlgorithm` to use EncodingAlgorithm
ReprLearner=@PredictiveRepresentationLearner

TrainerConfig.data_transformer_ctor=@agent/ImageScaleTransformer
Agent.representation_learner_cls=%ReprLearner
Agent.optimizer=@Adam()
agent/ImageScaleTransformer.min=0.0
agent/ImageScaleTransformer.fields=['observation.camera']

# Not yet able to successfully train with sparse reward.
suite_carla.Player.sparse_reward=False

# Cannot train well with allow_negative_distance_reward=False yet.
suite_carla.Player.allow_negative_distance_reward=True

# Currently, even a small penalty such as one make the training much worse
suite_carla.Player.max_collision_penalty=0.

# CarlaEnvironment.num_other_vehicles=20
# CarlaEnvironment.num_walkers=20

# training config
TrainerConfig.initial_collect_steps=3000
TrainerConfig.mini_batch_length=4
TrainerConfig.unroll_length=10
TrainerConfig.mini_batch_size=64
TrainerConfig.num_updates_per_train_iter=1
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False
TrainerConfig.num_iterations=1000000
TrainerConfig.num_checkpoints=20
TrainerConfig.evaluate=False
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.summary_interval=100
TrainerConfig.replay_buffer_length=90000
TrainerConfig.summarize_action_distributions=True
