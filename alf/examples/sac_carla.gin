include 'sac.gin'
include 'carla.gin'

import alf.algorithms.td_loss
import alf.algorithms.encoding_algorithm

encoding_dim = 256
fc_layers_params = (256,)
activation = torch.relu_

actor/StableNormalProjectionNetwork.state_dependent_std=True
actor/StableNormalProjectionNetwork.squash_mean=False
actor/StableNormalProjectionNetwork.scale_distribution=True
actor/StableNormalProjectionNetwork.min_std=1e-3
actor/StableNormalProjectionNetwork.max_std=10.
actor/ActorDistributionNetwork.fc_layer_params=%fc_layers_params
actor/ActorDistributionNetwork.activation=%activation
actor/ActorDistributionNetwork.use_fc_bn=%use_batch_normalization
actor/ActorDistributionNetwork.continuous_projection_net_ctor=@actor/StableNormalProjectionNetwork

critic/CriticNetwork.joint_fc_layer_params=%fc_layers_params
critic/CriticNetwork.activation=%activation
critic/CriticNetwork.use_fc_bn=%use_batch_normalization
critic/CriticNetwork.output_tensor_spec=@TensorSpec((%CarlaEnvironment.REWARD_DIMENSION,))

Adam.lr=1e-4
#Adam.eps=1e-12

SacAlgorithm.actor_network_cls=@actor/ActorDistributionNetwork
SacAlgorithm.critic_network_cls=@critic/CriticNetwork
SacAlgorithm.actor_optimizer=@Adam()
SacAlgorithm.critic_optimizer=@Adam()
SacAlgorithm.alpha_optimizer=@Adam()
sac/calc_default_target_entropy.min_prob=0.1
SacAlgorithm.target_entropy=@sac/calc_default_target_entropy
SacAlgorithm.target_update_tau=0.005
SacAlgorithm.critic_loss_ctor=@TDLoss
SacAlgorithm.use_parallel_network=True
SacAlgorithm.use_entropy_reward=False
SacAlgorithm.reward_weights=[1, 0., 0., 0.]

encoder/EncodingNetwork.input_preprocessors=%input_preprocessors
encoder/EncodingNetwork.preprocessing_combiner=@NestSum(activation=%activation, average=True)
encoder/EncodingNetwork.activation=%activation
encoder/EncodingNetwork.fc_layer_params=%fc_layers_params
encoder/EncodingAlgorithm.encoder_cls=@encoder/EncodingNetwork
ReprLearner=@encoder/EncodingAlgorithm

TrainerConfig.data_transformer_ctor=@agent/ImageScaleTransformer
Agent.representation_learner_cls=%ReprLearner
Agent.optimizer=@Adam()
agent/ImageScaleTransformer.min=0.0
agent/ImageScaleTransformer.fields=['observation.camera']

# Not yet able to successfully train with sparse reward.
suite_carla.Player.sparse_reward=False

# Currently, even a small penalty such as one make the training much worse
suite_carla.Player.max_collision_penalty=0.

# training config
TrainerConfig.initial_collect_steps=3000
TrainerConfig.mini_batch_length=4
TrainerConfig.unroll_length=10
TrainerConfig.mini_batch_size=64
TrainerConfig.num_updates_per_train_iter=1
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False
TrainerConfig.num_iterations=1000000
TrainerConfig.num_checkpoints=20
TrainerConfig.evaluate=False
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.summary_interval=100
TrainerConfig.replay_buffer_length=90000
TrainerConfig.summarize_action_distributions=True
