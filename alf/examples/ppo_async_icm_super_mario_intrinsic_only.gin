# Warning: Directly converted from the TF version, this gin file was NOT TESTED.

# icm on SuperMario using PPO algorithm
include 'ppo.gin'

import alf.environments.suite_mario

# environment config
# gym-retro>=0.7.0 is required for this experiment and also
#  a suitable `SuperMarioBros-Nes` rom should be obtain and imported (roms are not included in gym-retro)
#   see `https://retro.readthedocs.io/en/latest/getting_started.html#importing-roms` to import roms
create_environment.env_load_fn=@suite_mario.load
create_environment.env_name="SuperMarioBros-Nes"
create_environment.num_parallel_environments=64
suite_mario.load.state="Level1-1"


# algorithm config
CONV_LAYER_PARAMS=[(32,8,4),(64,4,2),(64,3,1)]

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.output_tensor_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(256,)
actor/ActorDistributionNetwork.activation_fn=@torch.nn.functional.elu_
actor/ActorDistributionNetwork.conv_layer_params=%CONV_LAYER_PARAMS
CategoricalProjectionNetwork.logits_init_output_factor=1e-10
actor/ActorDistributionNetwork.discrete_projection_net=@CategoricalProjectionNetwork


value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(256,256)
value/ValueNetwork.conv_layer_params=%CONV_LAYER_PARAMS
value/ValueNetwork.activation_fn=@torch.nn.functional.elu_

ac/Adam.learning_rate=3e-5

icm/encoding_net_fc_layer_params=(256,)
icm/EncodingNetwork.input_tensor_spec=%observation_spec
icm/EncodingNetwork.conv_layer_params=%CONV_LAYER_PARAMS
icm/EncodingNetwork.fc_layer_params=%icm/encoding_net_fc_layer_params
icm/EncodingNetwork.activation_fn=@torch.nn.functional.elu_
icm/TensorSpec.shape=%icm/encoding_net_fc_layer_params
ImageEncodingNetwork.same_padding=True

ICMAlgorithm.action_spec=%action_spec
ICMAlgorithm.feature_spec=@icm/TensorSpec()
ICMAlgorithm.encoding_net=@icm/EncodingNetwork()
ICMAlgorithm.hidden_size=256

ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
Agent.optimizer=@ac/Adam()
Agent.intrinsic_reward_module=@ICMAlgorithm()
Agent.gradient_clipping=10.0

# use only intrinsic reward for training policy
Agent.extrinsic_reward_coef=0.0
Agent.intrinsic_reward_coef=1.0

PPOLoss.entropy_regularization=1e-2
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss


# driver config
N = 2
AsyncOffPolicyDriver.num_actor_queues = %N
AsyncOffPolicyDriver.actor_queue_cap = 1
AsyncOffPolicyDriver.learn_queue_cap = 1

# training config
TrainerConfig.trainer=@async_off_policy_trainer
TrainerConfig.num_iterations = 100000
TrainerConfig.num_updates_per_train_iter = 2
TrainerConfig.unroll_length = 32
TrainerConfig.mini_batch_length = 1
TrainerConfig.mini_batch_size = 128
TrainerConfig.eval_interval = 100
TrainerConfig.evaluate = True
TrainerConfig.summarize_grads_and_vars = True
TrainerConfig.debug_summaries=1
TrainerConfig.summary_interval = 10
TrainerConfig.use_rollout_state = True
TrainerConfig.use_tf_functions = True
TrainerConfig.num_envs = %N

TrainerConfig.data_transformer_ctor=@ImageScaleTransformer
