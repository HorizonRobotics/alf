import alf.algorithms.functional_particle_vi_algorithm
import alf.trainers.policy_trainer

# dataset config
create_dataset.dataset_name='mnist'
create_dataset.train_batch_size=50
create_dataset.test_batch_size=100

# network architecture
CONV_LAYER_PARAMS = ((6, 5, 1, 2, 2), (16, 5, 1, 0, 2), (120, 5, 1))
FC_LAYER_PARAMS = ((84, True), )

# optimizer settings
func_parvi/Adam.lr = 1e-3
func_parvi/Adam.weight_decay = 1e-4
func_parvi_critic/Adam.lr = 1e-4
func_parvi_critic/Adam.weight_decay = 1e-4

# algorithm config
FuncParVI.fc_layer_params = %FC_LAYER_PARAMS
FuncParVI.hidden_layers = %HIDDEN_LAYERS
FuncParVI.num_particles = 10

FuncParVI.use_fc_bn = True
FuncParVI.par_vi = 'svgd3'
FuncParVI.loss_type = 'classification'
FuncParVI.entropy_regularization = 1.0
FuncParVI.optimizer = @func_parvi/Adam()
FuncParVI.critic_optimizer = @func_parvi_critic/Adam()
FuncParVI.critic_hidden_layers = (512,512)
FuncParVI.critic_iter_num = 5
FuncParVI.critic_l2_weight = 10.0
FuncParVI.logging_training = True
FuncParVI.logging_evaluate = True


# training config
TrainerConfig.algorithm_ctor=@FuncParVI
TrainerConfig.num_iterations=100
TrainerConfig.num_checkpoints=1
TrainerConfig.evaluate=True
TrainerConfig.eval_uncertainty=False
TrainerConfig.eval_interval=1
TrainerConfig.summary_interval=1
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.hold_out_dataset='mnist'
TrainerConfig.train_classes = [0, 1, 2, 3, 4, 5]
TrainerConfig.hold_out_classes = [6, 7, 8, 9]
