include 'ppo.gin'

# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
ActorCriticAlgorithm.reward_shaping_fn = @reward_scaling
common.reward_scaling.scale = 0.01

# algorithm config
PPOLoss.entropy_regularization=1e-4
PPOLoss.gamma=0.98
PPOLoss.td_error_loss_fn=@element_wise_huber_loss
PPOLoss.normalize_advantages=False
create_ac_algorithm.actor_fc_layers=(100,)
create_ac_algorithm.value_fc_layers=(100,)
create_ac_algorithm.learning_rate=0.001

# training config
OffPolicyTrainer.mini_batch_length=1
OffPolicyTrainer.unroll_length=32
OffPolicyTrainer.mini_batch_size=128
OffPolicyTrainer.num_updates_per_train_step=4
Trainer.num_iterations=1000
Trainer.checkpoint_interval=100000
Trainer.use_tf_functions=True
Trainer.evaluate=False
Trainer.debug_summaries=True
Trainer.summary_interval=5

TFUniformReplayBuffer.max_length = 2048

