include 'ppo.gin'

# environment config
create_environment.env_name="CartPole-v0"
create_environment.num_parallel_environments=8

# reward scaling
TrainerConfig.data_transformer_ctor = @RewardScaling
RewardScaling.scale = 0.01

# algorithm config
actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.output_tensor_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(100,)

value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.fc_layer_params=(100,)

ac/Adam.learning_rate=1e-3

ActorCriticAlgorithm.actor_network=@actor/ActorDistributionNetwork()
ActorCriticAlgorithm.value_network=@value/ValueNetwork()
Agent.optimizer=@ac/Adam()


PPOLoss.entropy_regularization=1e-4
PPOLoss.gamma=0.98
PPOLoss.td_error_loss_fn=@element_wise_huber_loss
PPOLoss.normalize_advantages=False

# training config
TrainerConfig.mini_batch_length=1
TrainerConfig.unroll_length=32
TrainerConfig.mini_batch_size=128
TrainerConfig.num_updates_per_train_iter=4
TrainerConfig.num_iterations=200
TrainerConfig.checkpoint_interval=100000
TrainerConfig.use_tf_functions=True
TrainerConfig.evaluate=True
TrainerConfig.eval_interval=50
TrainerConfig.debug_summaries=False
TrainerConfig.summary_interval=5

ReplayBuffer.max_length = 2048

