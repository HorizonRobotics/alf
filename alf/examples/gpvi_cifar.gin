import alf.algorithms.hypernetwork_algorithm
import alf.trainers.policy_trainer

# dataset config
create_dataset.dataset_name='cifar10'
create_dataset.train_batch_size=50
create_dataset.test_batch_size=100

CONV_LAYER_PARAMS = ((32, 3, 1, 0, 2), (64, 3, 1, 0, 2), (64, 3, 1, 0, 2))
FC_LAYER_PARAMS = ((128, True), )
HIDDEN_LAYERS = (512, 1024)

hypernet/Adam.lr = 1e-5
hypernet/Adam.weight_decay = 0 
hypernet_pinverse/Adam.lr = 1e-4

# algorithm config
HyperNetwork.conv_layer_params = %CONV_LAYER_PARAMS
HyperNetwork.fc_layer_params = %FC_LAYER_PARAMS
HyperNetwork.hidden_layers = %HIDDEN_LAYERS
HyperNetwork.noise_dim = 512
HyperNetwork.num_particles = 10

HyperNetwork.use_fc_bn = True
HyperNetwork.par_vi = 'svgd3'
HyperNetwork.functional_gradient='rkhs'
HyperNetwork.loss_type = 'classification'
HyperNetwork.entropy_regularization = 1e-5
HyperNetwork.force_fullrank=False
HyperNetwork.pinverse_hidden_size=256
HyperNetwork.optimizer = @hypernet/Adam()
HyperNetwork.pinverse_optimizer = @hypernet_pinverse/Adam()
               
HyperNetwork.logging_training = True
HyperNetwork.logging_evaluate = True

import alf.networks.param_networks
ParamConvNet.use_bias=True

# training config
TrainerConfig.algorithm_ctor=@HyperNetwork
TrainerConfig.num_iterations=100
TrainerConfig.num_checkpoints=1
TrainerConfig.evaluate=True
TrainerConfig.eval_uncertainty=True
TrainerConfig.eval_interval=1
TrainerConfig.summary_interval=1
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.hold_out_dataset = 'cifar10'
TrainerConfig.train_classes = [0, 1, 2, 3, 4, 5]
TrainerConfig.hold_out_classes = [6, 7, 8, 9]
TrainerConfig.random_seed = 123344
