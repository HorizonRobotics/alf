{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Stable gradient descent for Gaussian distribution\n","\n","## Wei Xu\n","\n","Gaussian distribution has two parameters $\\mu$ and $\\sigma$. Given a sample $x$\n","from the distribution, the negative log probability is\n","$$\n","\\begin{equation*}\n","l = \\frac{1}{2}\\log 2\\pi + \\log \\sigma + \\frac{1}{2}\\frac{(\\mu-x)^2}{\\sigma^2}\n","\\end{equation*}\n","$$\n","Its derivatives are:\n","$$\n","\\begin{array}{ll}\n","&& \\frac{\\partial l}{\\partial \\mu} = \\frac{\\mu-x}{\\sigma^2} \\\\\n","&& \\frac{\\partial l}{\\partial \\sigma}\n","    = \\frac{1}{\\sigma}-\\frac{(\\mu-x)^2}{\\sigma^3} \\\\\n","&& \\frac{\\partial^2l}{\\partial \\mu^2} = \\frac{1}{\\sigma^2} \\\\\n","&& \\frac{\\partial^2l}{\\partial \\sigma^2}\n","    = -\\frac{1}{\\sigma^2} + 3\\frac{(\\mu-x)^2}{\\sigma^4}\n","\\end{array}\n","$$\n","From these equations we can see that $\\frac{\\partial l}{\\partial \\mu}$ can\n","become very big if $\\sigma$ become very small. This cannot be fully compensated\n","by optimizers with adaptive learning rate such as ADAM [1](#ADAM). For ADAM,\n","the learning rate is the reciprocal of $v_t$. For $\\mu$ in our problem, the\n","learning rate is:\n","$$\n","\\begin{equation*}\n","\\frac{1}{v_t(\\mu)} = \\frac{1}{\\sqrt{O\\left(\\frac{(\\mu-x)^2}{\\sigma^4}\\right)}}\n"," = \\frac{1}{\\sqrt{O\\left(\\frac{1}{\\sigma^2}\\right)}}=O(\\sigma)\n","\\end{equation*}\n","$$\n","where we use the fact the $x$ is sampled from $N(\\mu, \\sigma)$ and hence\n","$\\mu-x=O(\\sigma)$. Optimization theory tells us that gradient descent will\n","diverge if the learning rate is larger than\n","$2/\\frac{\\partial^2l}{\\partial\\mu^2}=O(\\sigma^2)$\n","(See for example, section 5.1 in [2](#EfficientBackprop). From this, we can\n","see that the learning rate of ADAM is too high when $\\sigma$ is small and\n","leads to diverging behavior, which is exactly what we observed in our experiments.\n","\n","Our strategy is to parameterize Gaussian in a different way so that its second\n","derivatives are bounded when $\\sigma$ is small. Let $\\mu=\\alpha \\sigma$,\n","$\\sigma=\\frac{1}{\\log(1+\\exp(\\beta))}\\equiv\\frac{1}{\\gamma}$. Now we have\n","$$\n","\\begin{equation*}\n","l = \\frac{1}{2}\\log 2\\pi - \\log \\gamma + \\frac{1}{2}(\\alpha-x \\gamma)^2\n","\\end{equation*}\n","$$\n","The derivatives are:\n","$$\n","\\begin{array}{ll}\n","&& \\frac{\\partial l}{\\partial \\alpha} = \\alpha-x \\gamma \\\\\n","&& \\frac{\\partial l}{\\partial \\beta}\n","   = \\left(-\\frac{1}{\\gamma} + x(x\\gamma-\\alpha)\\right)\\frac{d\\gamma}{d\\beta} \\\\\n","&& \\frac{\\partial^2l}{\\partial \\alpha^2} = 1  \\\\\n","&& \\frac{\\partial ^2l}{\\partial \\beta^2}\n","   = \\left(\\frac{1}{\\gamma^2} + x^2\\right)\\left(\\frac{d\\gamma}{d\\beta}\\right)^2\n","   +\\left(-\\frac{1}{\\gamma} + x(x\\gamma-\\alpha)\\right) \\frac{d^2\\gamma}{d\\beta^2} \\\\\n","&& \\frac{d\\gamma}{d\\beta} = \\frac{\\exp(\\beta)}{1+\\exp(\\beta)} \\\\\n","&& \\frac{d^2\\gamma}{d\\beta^2} = \\frac{\\exp(\\beta)}{(1+\\exp(\\beta))^2} \\\\\n","\\end{array}\n","$$\n","\n","First, we see that $\\frac{\\partial l}{\\partial \\alpha}$ is bounded  because\n","$x\\gamma-\\alpha=O(1)$ (since x is sampled from the same distribution). Second,\n","because $\\frac{1}{\\gamma}\\frac{\\partial\\gamma}{\\partial\\beta}$ is bounded as\n","shown in the following:\n","$$\n","\\begin{array}{ll}\n","\\frac{1}{\\gamma}\\frac{\\partial\\gamma}{\\partial\\beta}\n"," &=& \\frac{1}{\\gamma}\\frac{\\exp(\\beta)}{1+\\exp(\\beta)}\n"," = \\frac{\\exp(\\gamma)-1}{\\gamma \\exp(\\gamma)}\n"," = \\frac{1-\\exp(-\\gamma)}{\\gamma} \\le 1\n","\\end{array}\n","$$\n","$\\frac{\\partial l}{\\partial \\beta}$ is also bounded.\n","Third, $\\frac{\\partial^2l}{\\partial \\alpha^2}$ is a constant, which is nice. And\n","finally, for $\\frac{\\partial^2l}{\\partial \\beta^2}$, we have:\n","$$\n","\\begin{equation*}\n","\\frac{\\partial^2l}{\\partial\\beta^2}\n","\\le \\left(\\left(\\frac{1}{\\gamma}\\frac{\\partial\\gamma}{\\partial\\beta}\\right)^2\n","     +x^2\\right) + |x(x\\gamma-\\alpha)|\n","=O(x^2+|x|)\n","\\end{equation*}\n","$$\n","hence it is also bounded.\n","\n","# References\n","<a name=\"ADAM\"></a> [1] D.P. Kingma and J. L. Ba _ADAM: A Method for Stochastic\n"," Optimization_ ICLR 2015, arXiv:1412.6980\n","\n","<a name=\"EfficientBackprop\"></a> [2] Y. LeCun, L. Bottou, G. Orr and K. Muller\n"," _Efficient Backprop_ in Orr, G. and Muller K. (Eds), Neural Networks: Tricks of\n","  the trade, Springer, 1998"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}